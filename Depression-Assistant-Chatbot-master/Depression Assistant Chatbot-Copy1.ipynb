{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['how', 'the', 'hell', 'who', 'the', 'heck', 'moved', 'my', 'fridge', 'should', 'i', 'knock', 'the', 'landlord', 'door', 'angry', 'mad'], 'anger')\n",
      "760\n",
      "995\n",
      "673\n",
      "714\n",
      "4689\n",
      "0\n",
      "['anger', 'fear', 'sadness', 'joy']\n",
      "(3751, 7580)\n",
      "(3751, 4)\n",
      "(938, 7580)\n",
      "(938, 4)\n",
      " Shape of X is  (7580, 3751)\n",
      " Shape of Y is  (4, 3751)\n",
      " Shape of m is  3751\n",
      " Shape of W1 is  (100, 7580)\n",
      " Shape of W2 is  (4, 100)\n",
      "################### TRAIN MODEL STATISTICS ######################\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.24      0.32      1826\n",
      "           1       0.25      0.26      0.25      1011\n",
      "           2       0.03      0.14      0.05       213\n",
      "           3       0.12      0.15      0.13       701\n",
      "\n",
      "    accuracy                           0.22      3751\n",
      "   macro avg       0.22      0.20      0.19      3751\n",
      "weighted avg       0.32      0.22      0.25      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.84      0.72       707\n",
      "           1       0.90      0.54      0.67      1775\n",
      "           2       0.50      0.75      0.60       574\n",
      "           3       0.60      0.76      0.67       695\n",
      "\n",
      "    accuracy                           0.67      3751\n",
      "   macro avg       0.66      0.72      0.67      3751\n",
      "weighted avg       0.73      0.67      0.67      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.74      0.81      1147\n",
      "           1       0.86      0.80      0.83      1138\n",
      "           2       0.67      0.94      0.78       618\n",
      "           3       0.80      0.84      0.82       848\n",
      "\n",
      "    accuracy                           0.81      3751\n",
      "   macro avg       0.81      0.83      0.81      3751\n",
      "weighted avg       0.83      0.81      0.81      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.94      0.85       781\n",
      "           1       0.92      0.77      0.84      1266\n",
      "           2       0.77      0.90      0.83       744\n",
      "           3       0.89      0.82      0.85       960\n",
      "\n",
      "    accuracy                           0.84      3751\n",
      "   macro avg       0.84      0.86      0.84      3751\n",
      "weighted avg       0.85      0.84      0.84      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.69      0.80      1338\n",
      "           1       0.79      0.99      0.88       845\n",
      "           2       0.80      0.91      0.85       763\n",
      "           3       0.83      0.91      0.87       805\n",
      "\n",
      "    accuracy                           0.85      3751\n",
      "   macro avg       0.85      0.87      0.85      3751\n",
      "weighted avg       0.86      0.85      0.84      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      1.00      0.81       649\n",
      "           1       0.98      0.69      0.81      1489\n",
      "           2       0.77      0.92      0.84       720\n",
      "           3       0.89      0.88      0.88       893\n",
      "\n",
      "    accuracy                           0.83      3751\n",
      "   macro avg       0.83      0.87      0.84      3751\n",
      "weighted avg       0.86      0.83      0.83      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76      1537\n",
      "           1       0.75      1.00      0.86       796\n",
      "           2       0.77      0.94      0.85       707\n",
      "           3       0.79      0.98      0.87       711\n",
      "\n",
      "    accuracy                           0.82      3751\n",
      "   macro avg       0.82      0.88      0.83      3751\n",
      "weighted avg       0.86      0.82      0.82      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80       630\n",
      "           1       0.98      0.67      0.80      1549\n",
      "           2       0.76      0.94      0.84       694\n",
      "           3       0.89      0.90      0.90       878\n",
      "\n",
      "    accuracy                           0.83      3751\n",
      "   macro avg       0.83      0.88      0.83      3751\n",
      "weighted avg       0.87      0.83      0.83      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.65      0.79      1442\n",
      "           1       0.76      1.00      0.86       802\n",
      "           2       0.81      0.92      0.86       767\n",
      "           3       0.82      0.98      0.89       740\n",
      "\n",
      "    accuracy                           0.84      3751\n",
      "   macro avg       0.85      0.89      0.85      3751\n",
      "weighted avg       0.87      0.84      0.84      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      1.00      0.84       682\n",
      "           1       0.98      0.78      0.87      1339\n",
      "           2       0.83      0.93      0.88       777\n",
      "           3       0.94      0.88      0.91       953\n",
      "\n",
      "    accuracy                           0.87      3751\n",
      "   macro avg       0.87      0.89      0.87      3751\n",
      "weighted avg       0.89      0.87      0.87      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.78      0.87      1199\n",
      "           1       0.83      0.99      0.90       879\n",
      "           2       0.88      0.91      0.89       836\n",
      "           3       0.91      0.96      0.94       837\n",
      "\n",
      "    accuracy                           0.90      3751\n",
      "   macro avg       0.90      0.91      0.90      3751\n",
      "weighted avg       0.91      0.90      0.90      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93       853\n",
      "           1       0.97      0.89      0.93      1149\n",
      "           2       0.89      0.94      0.91       824\n",
      "           3       0.96      0.92      0.94       925\n",
      "\n",
      "    accuracy                           0.93      3751\n",
      "   macro avg       0.93      0.93      0.93      3751\n",
      "weighted avg       0.93      0.93      0.93      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.88      0.93      1059\n",
      "           1       0.92      0.98      0.95       990\n",
      "           2       0.90      0.94      0.92       827\n",
      "           3       0.95      0.96      0.95       875\n",
      "\n",
      "    accuracy                           0.94      3751\n",
      "   macro avg       0.94      0.94      0.94      3751\n",
      "weighted avg       0.94      0.94      0.94      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95       933\n",
      "           1       0.96      0.94      0.95      1085\n",
      "           2       0.91      0.94      0.93       832\n",
      "           3       0.97      0.95      0.96       901\n",
      "\n",
      "    accuracy                           0.95      3751\n",
      "   macro avg       0.95      0.95      0.95      3751\n",
      "weighted avg       0.95      0.95      0.95      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95      1008\n",
      "           1       0.94      0.97      0.96      1026\n",
      "           2       0.91      0.95      0.93       832\n",
      "           3       0.96      0.96      0.96       885\n",
      "\n",
      "    accuracy                           0.95      3751\n",
      "   macro avg       0.95      0.95      0.95      3751\n",
      "weighted avg       0.95      0.95      0.95      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       961\n",
      "           1       0.96      0.96      0.96      1060\n",
      "           2       0.92      0.95      0.94       836\n",
      "           3       0.97      0.96      0.96       894\n",
      "\n",
      "    accuracy                           0.96      3751\n",
      "   macro avg       0.95      0.96      0.95      3751\n",
      "weighted avg       0.96      0.96      0.96      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       985\n",
      "           1       0.95      0.97      0.96      1042\n",
      "           2       0.92      0.96      0.94       835\n",
      "           3       0.97      0.96      0.96       889\n",
      "\n",
      "    accuracy                           0.96      3751\n",
      "   macro avg       0.96      0.96      0.96      3751\n",
      "weighted avg       0.96      0.96      0.96      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       970\n",
      "           1       0.96      0.96      0.96      1057\n",
      "           2       0.93      0.96      0.94       836\n",
      "           3       0.97      0.97      0.97       888\n",
      "\n",
      "    accuracy                           0.96      3751\n",
      "   macro avg       0.96      0.96      0.96      3751\n",
      "weighted avg       0.96      0.96      0.96      3751\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96       978\n",
      "           1       0.96      0.97      0.97      1050\n",
      "           2       0.93      0.96      0.95       842\n",
      "           3       0.97      0.97      0.97       881\n",
      "\n",
      "    accuracy                           0.96      3751\n",
      "   macro avg       0.96      0.96      0.96      3751\n",
      "weighted avg       0.96      0.96      0.96      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97       973\n",
      "           1       0.97      0.97      0.97      1055\n",
      "           2       0.94      0.96      0.95       845\n",
      "           3       0.97      0.98      0.98       878\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.96      0.97      0.97      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97       977\n",
      "           1       0.97      0.97      0.97      1051\n",
      "           2       0.94      0.96      0.95       846\n",
      "           3       0.97      0.98      0.98       877\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.97      0.97      0.97      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       971\n",
      "           1       0.97      0.97      0.97      1053\n",
      "           2       0.95      0.96      0.95       851\n",
      "           3       0.97      0.98      0.98       876\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.97      0.97      0.97      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       970\n",
      "           1       0.97      0.97      0.97      1053\n",
      "           2       0.95      0.96      0.95       852\n",
      "           3       0.97      0.98      0.98       876\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.97      0.97      0.97      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       968\n",
      "           1       0.97      0.98      0.97      1051\n",
      "           2       0.95      0.96      0.96       855\n",
      "           3       0.98      0.98      0.98       877\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.97      0.97      0.97      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97       966\n",
      "           1       0.97      0.98      0.97      1053\n",
      "           2       0.95      0.96      0.96       856\n",
      "           3       0.98      0.98      0.98       876\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.97      0.97      0.97      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       966\n",
      "           1       0.97      0.98      0.97      1053\n",
      "           2       0.95      0.96      0.96       857\n",
      "           3       0.98      0.99      0.98       875\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.97      0.97      0.97      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       967\n",
      "           1       0.97      0.98      0.98      1051\n",
      "           2       0.95      0.96      0.96       855\n",
      "           3       0.98      0.99      0.98       878\n",
      "\n",
      "    accuracy                           0.97      3751\n",
      "   macro avg       0.97      0.97      0.97      3751\n",
      "weighted avg       0.97      0.97      0.97      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       964\n",
      "           1       0.97      0.98      0.98      1053\n",
      "           2       0.95      0.97      0.96       854\n",
      "           3       0.98      0.99      0.99       880\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       962\n",
      "           1       0.98      0.98      0.98      1054\n",
      "           2       0.96      0.97      0.96       855\n",
      "           3       0.98      0.99      0.99       880\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       960\n",
      "           1       0.98      0.98      0.98      1056\n",
      "           2       0.96      0.97      0.96       856\n",
      "           3       0.98      0.99      0.99       879\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       961\n",
      "           1       0.98      0.98      0.98      1055\n",
      "           2       0.96      0.97      0.96       856\n",
      "           3       0.98      0.99      0.99       879\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       961\n",
      "           1       0.98      0.98      0.98      1055\n",
      "           2       0.96      0.97      0.96       855\n",
      "           3       0.98      0.99      0.99       880\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       960\n",
      "           1       0.98      0.98      0.98      1054\n",
      "           2       0.96      0.97      0.97       857\n",
      "           3       0.98      0.99      0.99       880\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       959\n",
      "           1       0.98      0.98      0.98      1053\n",
      "           2       0.96      0.97      0.97       859\n",
      "           3       0.98      0.99      0.99       880\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       958\n",
      "           1       0.98      0.98      0.98      1053\n",
      "           2       0.97      0.97      0.97       861\n",
      "           3       0.98      0.99      0.99       879\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       958\n",
      "           1       0.98      0.98      0.98      1053\n",
      "           2       0.97      0.97      0.97       861\n",
      "           3       0.98      0.99      0.99       879\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       958\n",
      "           1       0.98      0.98      0.98      1051\n",
      "           2       0.97      0.97      0.97       863\n",
      "           3       0.99      0.99      0.99       879\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       957\n",
      "           1       0.98      0.98      0.98      1051\n",
      "           2       0.97      0.97      0.97       864\n",
      "           3       0.99      0.99      0.99       879\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       957\n",
      "           1       0.98      0.98      0.98      1052\n",
      "           2       0.97      0.97      0.97       863\n",
      "           3       0.99      0.99      0.99       879\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       957\n",
      "           1       0.98      0.98      0.98      1052\n",
      "           2       0.97      0.97      0.97       863\n",
      "           3       0.99      0.99      0.99       879\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       954\n",
      "           1       0.98      0.99      0.98      1052\n",
      "           2       0.97      0.97      0.97       865\n",
      "           3       0.99      0.99      0.99       880\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       954\n",
      "           1       0.98      0.99      0.98      1052\n",
      "           2       0.97      0.97      0.97       865\n",
      "           3       0.99      0.99      0.99       880\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       954\n",
      "           1       0.98      0.99      0.98      1052\n",
      "           2       0.97      0.97      0.97       864\n",
      "           3       0.99      0.99      0.99       881\n",
      "\n",
      "    accuracy                           0.98      3751\n",
      "   macro avg       0.98      0.98      0.98      3751\n",
      "weighted avg       0.98      0.98      0.98      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       953\n",
      "           1       0.98      0.99      0.98      1051\n",
      "           2       0.98      0.97      0.98       865\n",
      "           3       0.99      0.99      0.99       882\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.99      0.98      0.98      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       953\n",
      "           1       0.98      0.99      0.99      1050\n",
      "           2       0.98      0.97      0.98       866\n",
      "           3       0.99      0.99      0.99       882\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.99      0.99      0.99      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       952\n",
      "           1       0.98      0.99      0.99      1050\n",
      "           2       0.98      0.97      0.98       866\n",
      "           3       0.99      0.99      0.99       883\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.99      0.99      0.99      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       952\n",
      "           1       0.98      0.99      0.99      1049\n",
      "           2       0.98      0.98      0.98       866\n",
      "           3       0.99      0.99      0.99       884\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.99      0.99      0.99      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       952\n",
      "           1       0.98      0.99      0.99      1049\n",
      "           2       0.98      0.98      0.98       866\n",
      "           3       0.99      0.99      0.99       884\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.99      0.99      0.99      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       954\n",
      "           1       0.98      0.99      0.99      1049\n",
      "           2       0.98      0.98      0.98       864\n",
      "           3       0.99      0.99      0.99       884\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.99      0.99      0.99      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       954\n",
      "           1       0.98      0.99      0.99      1049\n",
      "           2       0.98      0.98      0.98       864\n",
      "           3       0.99      0.99      0.99       884\n",
      "\n",
      "    accuracy                           0.99      3751\n",
      "   macro avg       0.99      0.99      0.99      3751\n",
      "weighted avg       0.99      0.99      0.99      3751\n",
      "\n",
      "saved synapses to: weights.json\n",
      " Shape of X is  (7580, 938)\n",
      " Shape of Y is  (4, 938)\n",
      " Shape of m is  938\n",
      "################### TEST MODEL STATISTICS ######################\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       230\n",
      "           1       0.93      0.96      0.94       278\n",
      "           2       0.91      0.88      0.90       200\n",
      "           3       0.95      0.97      0.96       230\n",
      "\n",
      "    accuracy                           0.93       938\n",
      "   macro avg       0.93      0.93      0.93       938\n",
      "weighted avg       0.93      0.93      0.93       938\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "# For making a precision, recall report and confusion matrix on the classes\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from string import punctuation\n",
    "anger_training_set = []\n",
    "fear_training_set = []\n",
    "sadness_training_set = []\n",
    "joy_training_set = []\n",
    "\n",
    "anger_test_set = []\n",
    "fear_test_set = []\n",
    "sadness_test_set = []\n",
    "joy_test_set = []\n",
    "stemmer = LancasterStemmer()\n",
    "all_words=[]\n",
    "\n",
    "# Here I am loading the dataset from stored folder. The training data is stored as text file and each tweet is accompanied\n",
    "# by the magnitude of its sentiment (0 to 1). I had to go through the tweets myself and observed that a threshold of 0.5 is \n",
    "# good enough to classify a tweet according to its sentiment. Tweets with lesser threshold were not definitive to be trained as per their mentioned classification  \n",
    "# I only read those tweets that have a dominant classification factor i.e. above 0.5\n",
    "# Here i am setting each tweet's threshold magnitude accordingly\n",
    "def load_training_data(sentiment):\n",
    "    data = open(r\"C:\\Users\\HP\\Documents\\s4--prjct-sample\\Depression-Assistant-Chatbot-master\\\\\"+sentiment+\"_training_set.txt\",encoding=\"utf8\")\n",
    "    if sentiment == \"anger\":        \n",
    "        threshold = 0.5\n",
    "    elif sentiment == \"fear\":\n",
    "        threshold = 0.6\n",
    "    elif sentiment == \"sadness\":\n",
    "        threshold = 0.5\n",
    "    elif sentiment == \"joy\":\n",
    "        threshold = 0.5\n",
    "    else:\n",
    "        pass\n",
    "    return data,threshold\n",
    "\n",
    "\n",
    "def load_test_data(sentiment):\n",
    "   # encoding = 'raw_unicode_escape'\n",
    "    data = open(r\"C:\\Users\\HP\\Documents\\s4--prjct-sample\\Depression-Assistant-Chatbot-master\\\\\"+sentiment+\"_test_set.txt\",encoding=\"utf8\")\n",
    "    return data\n",
    "\n",
    "\n",
    "# In this method, I am cleaning the tweet data removing punctuations and then tokenizing the words in tweet removing name tags\n",
    "# and appending them to training set\n",
    "def clean_data(training_data,threshold):\n",
    "    training_set = []\n",
    "    for line in training_data:\n",
    "        line = line.strip().lower()\n",
    "        if line.split()[-1] == \"none\":\n",
    "            line = \" \".join(filter(lambda x:x[0]!='@', line.split()))\n",
    "            punct = line.maketrans(\"\",\"\",'.*%$^0123456789#!][\\?&/)/(+-<>')\n",
    "            result = line.translate(punct)\n",
    "            tokened_sentence = nltk.word_tokenize(result)\n",
    "            sentence = tokened_sentence[0:len(tokened_sentence)-1]\n",
    "            label = tokened_sentence[-2]\n",
    "            training_set.append((sentence,label))\n",
    "        else:\n",
    "            intensity = float(line.split()[-1])        \n",
    "            if (intensity>=threshold):\n",
    "                line = \" \".join(filter(lambda x:x[0]!='@', line.split()))\n",
    "                punct = line.maketrans(\"\",\"\",'.*%$^0123456789#!][\\?&/)/(+-<>')\n",
    "                result = line.translate(punct)\n",
    "                tokened_sentence = nltk.word_tokenize(result)\n",
    "                sentence = tokened_sentence[0:len(tokened_sentence)-1]\n",
    "                label = tokened_sentence[-1]\n",
    "                training_set.append((sentence,label))\n",
    "    return training_set\n",
    "    \n",
    "# This method collects all the unique words that are contained in the entire tweet dataset, finds their stem and \n",
    "# encodes each sentence according to the bag of words appending it to training set\n",
    "def bag_of_words(all_data):\n",
    "    training_set = []\n",
    "    all_words = []\n",
    "    for each_list in all_data:\n",
    "        for words in each_list[0]:\n",
    "            word = stemmer.stem(words)\n",
    "            all_words.append(word)\n",
    "    all_words = list(set(all_words))\n",
    "    \n",
    "    for each_sentence in all_data:  \n",
    "        bag = [0]*len(all_words)\n",
    "        training_set.append(encode_sentence(all_words,each_sentence[0],bag))\n",
    "    return training_set,all_words\n",
    "\n",
    "# Here we encode each tweet's words according to the words it contained from the bag of words which is based on all words in all tweets\n",
    "def encode_sentence(all_words,sentence, bag):\n",
    "    for s in sentence:        \n",
    "        stemmed_word = stemmer.stem(s)\n",
    "        for i,word in enumerate(all_words):\n",
    "            if stemmed_word == word:\n",
    "                bag[i] = 1\n",
    "    return bag\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    bag = [] \n",
    "    all_data = []\n",
    "    all_test_data = []\n",
    "    labels = []\n",
    "    classes = []\n",
    "    labels = []\n",
    "    test_labels = []\n",
    "    words=[]\n",
    "    test_words = []\n",
    "        \n",
    "    ######### Here we read the whole training data for each class and the threshold we will use for its classification\n",
    "    anger_training_data,threshold = load_training_data(\"anger\")\n",
    "    anger_training_set = clean_data(anger_training_data,threshold)\n",
    "    print(anger_training_set[0])\n",
    "    \n",
    "    fear_training_data,threshold = load_training_data(\"fear\")\n",
    "    fear_training_set = clean_data(fear_training_data,threshold)\n",
    "    \n",
    "    sadness_training_data,threshold = load_training_data(\"sadness\")\n",
    "    sadness_training_set = clean_data(sadness_training_data,threshold)\n",
    "    \n",
    "    joy_training_data,threshold = load_training_data(\"joy\")\n",
    "    joy_training_set = clean_data(joy_training_data,threshold)\n",
    "    \n",
    "    \n",
    "    ######### Here we read the whole test data for each class and the threshold we will use for its classification\n",
    "    anger_test_data = load_test_data(\"anger\")\n",
    "    anger_test_set = clean_data(anger_test_data,threshold)\n",
    "    #print(anger_test_set[0])\n",
    "    print(len(anger_test_set))\n",
    "    \n",
    "    fear_test_data = load_test_data(\"fear\")\n",
    "    fear_test_set = clean_data(fear_test_data,threshold)\n",
    "   # print(fear_test_set[0])\n",
    "    print(len(fear_test_set))\n",
    "    \n",
    "    sadness_test_data = load_test_data(\"sadness\")\n",
    "    sadness_test_set = clean_data(sadness_test_data,threshold)\n",
    "  #  print(sadness_test_set[0])\n",
    "    print(len(sadness_test_set))\n",
    "    \n",
    "    joy_test_data = load_test_data(\"joy\")\n",
    "    joy_test_set = clean_data(joy_test_data,threshold)\n",
    "  #  print(joy_test_set[0])\n",
    "    print(len(joy_test_set))\n",
    "    ###### In every training set above we have a nested list whose first element is sentence and 2nd element its respective label ######\n",
    "    \n",
    "#    print(anger_training_set[0][0],anger_training_set[0][1])\n",
    "#    print(joy_training_set[0][0],joy_training_set[0][1])\n",
    "\n",
    "    \n",
    "    ###### Here we combine all training sets in one list ######\n",
    "    all_data.extend(anger_training_set)\n",
    "    all_data.extend(fear_training_set)\n",
    "    all_data.extend(sadness_training_set)\n",
    "    all_data.extend(joy_training_set)\n",
    "    \n",
    "    all_data.extend(anger_test_set)\n",
    "    all_data.extend(fear_test_set)\n",
    "    all_data.extend(sadness_test_set)\n",
    "    all_data.extend(joy_test_set)\n",
    "    \n",
    "    ###### Here we simply make a classification label list encoding our 4 classes as follows\n",
    "    \n",
    "    \n",
    "    for i,j in all_data:\n",
    "        if j == \"anger\":            \n",
    "            labels.append([1,0,0,0])\n",
    "        elif j == \"fear\":            \n",
    "            labels.append([0,1,0,0])\n",
    "        elif j == \"sadness\":            \n",
    "            labels.append([0,0,1,0])\n",
    "        elif j == \"joy\":            \n",
    "            labels.append([0,0,0,1])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    print(len(labels))\n",
    "    print(len(test_labels))\n",
    "    classes = [\"anger\",\"fear\",\"sadness\",\"joy\"]\n",
    "    print(classes)\n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "    \n",
    "    # Here we will have the whole training set and the all the words contained in whole training set\n",
    "    training_set,words = bag_of_words(all_data)\n",
    "    \n",
    "    # We convert our training,test set and training, test labels in a numpy array as it is required for calculations in neural net\n",
    "    dataset = np.array(training_set)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # It is important to shuffle dataset so your classifier does not attempt to memorize training set, this functions shuffles data and labels.\n",
    "    shuffling_function = np.random.permutation(dataset.shape[0])\n",
    "    shuffled_dataset, shuffled_labels = np.zeros((dataset.shape)),np.zeros((dataset.shape))\n",
    "    shuffled_dataset,shuffled_labels = dataset[shuffling_function],labels[shuffling_function]\n",
    "    \n",
    "    \n",
    "    split = int(len(shuffled_dataset)*0.8)\n",
    "    training_data = shuffled_dataset[:split]\n",
    "    training_labels = shuffled_labels[:split]\n",
    "    test_data = shuffled_dataset[split:]\n",
    "    test_labels = shuffled_labels[split:]\n",
    "    print(training_data.shape)\n",
    "    print(training_labels.shape)    \n",
    "    print(test_data.shape)\n",
    "    print(test_labels.shape)\n",
    "    \n",
    "        \n",
    "    ############# HERE WE HAVE A SHUFFLED DATASET WITH RESPECTIVE LABELS NOW WE HAVE TO TRAIN THIS DATA BOTH NUMPY ARRAYS ############\n",
    "    Train_model(training_data,training_labels,words,classes)\n",
    "    Test_model(test_data,test_labels,words,classes)\n",
    "\n",
    "# Method for calculating sigmoid\n",
    "def sigmoid(z):\n",
    "    return (1/(1+np.exp(-z)))\n",
    "    \n",
    "# Method for calculating relu\n",
    "def relu(z):\n",
    "    A = np.array(z,copy=True)\n",
    "    A[z<0]=0\n",
    "    assert A.shape == z.shape\n",
    "    return A\n",
    "    \n",
    "# Method for calculating softmax\n",
    "def softmax(x):\n",
    "    num = np.exp(x-np.amax(x,axis=0,keepdims=True))    \n",
    "    return num/np.sum(num,axis=0,keepdims=True)\n",
    "\n",
    "# Method for calculating forward propagation\n",
    "def forward_prop(n_x,n_h,n_y,m,X,W1,W2,b1,b2):\n",
    "    # Forward propagate data ... dimensions should be 100x1547\n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2,A1)+b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1,A1,Z2,A2\n",
    "\n",
    "# Method for calculating relu activation's derivative\n",
    "def relu_backward(da,dz):\n",
    "    da1 = np.array(da,copy=True)\n",
    "    da1[dz<0]=0\n",
    "    assert da1.shape == dz.shape\n",
    "    return da1\n",
    "\n",
    "# Method for calculating linear part of backward propagation\n",
    "def linear_backward(dz,a,m,w,b):\n",
    "    dw = (1/m)*np.dot(dz,a.T)\n",
    "    db = (1/m)*np.sum(dz,axis=1,keepdims=True)\n",
    "    da = np.dot(w.T,dz)\n",
    "    assert (dw.shape==w.shape)\n",
    "    assert (da.shape==a.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    return da,dw,db \n",
    "\n",
    "# Method for calculating loss function\n",
    "def calculate_loss(Y,Yhat,m):\n",
    "    loss = (-1/m)*np.sum(np.multiply(Y,np.log(Yhat)))\n",
    "    return loss\n",
    "\n",
    "# Method for back propagation\n",
    "def back_prop(Z1,A1,Z2,A2,X,Y,W1,W2,b1,b2,learning_rate,m):\n",
    "    dZ2 = A2-Y\n",
    "    da1,dw2,db2 = linear_backward(dZ2,A1,m,W2,b2)\n",
    "    dZ1 = relu_backward(da1,Z1)\n",
    "    da0,dw1,db1 = linear_backward(dZ1,X,m,W1,b1)\n",
    "    W2 = W2 - learning_rate * dw2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    W1 = W1 - learning_rate * dw1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    return W1,b1,W2,b2\n",
    "\n",
    "\n",
    "# Method for training model\n",
    "def Test_model(test_data, test_labels,words,classes):\n",
    "    all_losses = []\n",
    "    learning_rate = 0.1\n",
    "    iterations = 50\n",
    "    np.random.seed(1)\n",
    "    X = test_data.T\n",
    "    print(\" Shape of X is \", X.shape)\n",
    "    Y = test_labels.T\n",
    "    print(\" Shape of Y is \", Y.shape)\n",
    "    # m is total number of training examples\n",
    "    m = X.shape[1]\n",
    "    print(\" Shape of m is \", m)\n",
    "    # Number of hidden layer neurons\n",
    "    n_h = 100\n",
    "    # Number of training points\n",
    "    n_x = X.shape[0]\n",
    "    # Number of output neurons because we have 4 classes\n",
    "    n_y = 4\n",
    "    \n",
    "    weights_file = 'weights.json' \n",
    "    with open(weights_file) as data_file: \n",
    "        weights = json.load(data_file) \n",
    "        W1 = np.asarray(weights['weight1']) \n",
    "        W2 = np.asarray(weights['weight2'])\n",
    "        b1 = np.asarray(weights['bias1']) \n",
    "        b2 = np.asarray(weights['bias2'])\n",
    "\n",
    "    print(\"################### TEST MODEL STATISTICS ######################\")\n",
    "    for i in range(1):\n",
    "        # input layer is our encoded sentence\n",
    "        l0 = X\n",
    "        # matrix multiplication of input and hidden layer\n",
    "        l1 = relu(np.dot(W1,l0)+b1)\n",
    "        # output layer\n",
    "        l2 = softmax(np.dot(W2,l1)+b2)\n",
    "        predictions = np.argmax(l2, axis=0)\n",
    "        labels = np.argmax(Y, axis=0)\n",
    "        print(classification_report(predictions,labels))\n",
    "\n",
    "\n",
    "\n",
    "# Method for training model\n",
    "def Train_model(training_data, training_labels,words,classes):\n",
    "    all_losses = []\n",
    "    learning_rate = 0.1\n",
    "    iterations = 50\n",
    "    np.random.seed(1)\n",
    "    X = training_data.T\n",
    "    print(\" Shape of X is \", X.shape)\n",
    "    Y = training_labels.T\n",
    "    print(\" Shape of Y is \", Y.shape)\n",
    "    # m is total number of training examples\n",
    "    m = X.shape[1]\n",
    "    print(\" Shape of m is \", m)\n",
    "    # Number of hidden layer neurons\n",
    "    n_h = 100\n",
    "    # Number of training points\n",
    "    n_x = X.shape[0]\n",
    "    # Number of output neurons because we have 4 classes\n",
    "    n_y = 4\n",
    "    # Multiplying by 0.01 so that we get smaller weights .. dimensions 100x3787\n",
    "    W1 = np.random.randn(n_h,n_x)*0.01\n",
    "    print(\" Shape of W1 is \", W1.shape)\n",
    "    # Dimensions 100x1\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    # Dimensions 1547 x 4\n",
    "    W2 = np.random.randn(n_y,n_h)\n",
    "    print(\" Shape of W2 is \", W2.shape)\n",
    "    # Forward propagate data ... dimensions should be 100x1547\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    print(\"################### TRAIN MODEL STATISTICS ######################\")\n",
    "    for i in range(0,iterations):\n",
    "        Z1,A1,Z2,A2 = forward_prop(n_x,n_h,n_y,m,X,W1,W2,b1,b2)\n",
    "        predictions = np.argmax(A2, axis=0)\n",
    "        labels = np.argmax(Y, axis=0)\n",
    "        print(classification_report(predictions,labels))\n",
    "        Loss = calculate_loss(Y,A2,m)\n",
    "        W1,b1,W2,b2 = back_prop(Z1,A1,Z2,A2,X,Y,W1,W2,b1,b2,learning_rate,m)\n",
    "        all_losses.append(Loss)\n",
    "\n",
    "    # storing weights so that we can reuse them without having to retrain the neural network\n",
    "    weights = {'weight1': W1.tolist(), 'weight2': W2.tolist(), \n",
    "               'bias1':b1.tolist(), 'bias2':b2.tolist(),\n",
    "               'words': words,\n",
    "               'classes': classes\n",
    "              }\n",
    "    weights_file = \"weights.json\"\n",
    "\n",
    "    with open(weights_file, 'w') as outfile:\n",
    "        json.dump(weights, outfile, indent=4, sort_keys=True)\n",
    "    print (\"saved synapses to:\", weights_file)\n",
    "    plt.plot(all_losses)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to kill everyone @Name1 #why? \n",
      " classification: [['anger', array([0.4650087])], ['joy', array([0.20963463])], ['fear', array([0.19975792])], ['sadness', array([0.12559876])]] \n",
      "\n",
      "I am so happy @Name2 #yayyyy \n",
      " classification: [['joy', array([0.66784596])], ['anger', array([0.12870678])], ['fear', array([0.10337866])], ['sadness', array([0.10006859])]] \n",
      "\n",
      "This depression will kill me someday .. i am dying @Name3 #killme \n",
      " classification: [['sadness', array([0.5594182])], ['fear', array([0.2391959])], ['anger', array([0.11535852])]] \n",
      "\n",
      "I am afraid terrorists might attack us @Name4 #isis \n",
      " classification: [['fear', array([0.94459539])]] \n",
      "\n",
      "What should I do when i am happy @Name5  \n",
      " classification: [['joy', array([0.44340225])], ['fear', array([0.21579209])], ['anger', array([0.17826564])], ['sadness', array([0.16254002])]] \n",
      "\n",
      "I want to be happy \n",
      " classification: [['joy', array([0.68291732])], ['sadness', array([0.12885007])], ['anger', array([0.10671329])]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['joy', array([0.68291732])],\n",
       " ['sadness', array([0.12885007])],\n",
       " ['anger', array([0.10671329])]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability threshold\n",
    "ERROR_THRESHOLD = 0.1\n",
    "# load our calculated weight values\n",
    "weights_file = 'weights.json' \n",
    "with open(weights_file) as data_file: \n",
    "    weights = json.load(data_file) \n",
    "    W1 = np.asarray(weights['weight1']) \n",
    "    W2 = np.asarray(weights['weight2'])\n",
    "    b1 = np.asarray(weights['bias1']) \n",
    "    b2 = np.asarray(weights['bias2'])\n",
    "    all_words = weights['words']\n",
    "    classes = weights['classes']\n",
    "    \n",
    "def clean_sentence(verification_data):\n",
    "    line = verification_data\n",
    "    # Remove whitespace from line and lower case iter\n",
    "    line = line.strip().lower()\n",
    "    # Removing word with @ sign as we dont need name tags of twitter\n",
    "    line = \" \".join(filter(lambda x:x[0]!='@', line.split()))\n",
    "    # Remove punctuations and numbers from the line\n",
    "    punct = line.maketrans(\"\",\"\",'.*%$^0123456789#!][\\?&/)/(+-<>')\n",
    "    result = line.translate(punct)\n",
    "    # Tokenize the whole tweet sentence\n",
    "    tokened_sentence = nltk.word_tokenize(result)\n",
    "    # We take the tweet sentence from tokened sentence\n",
    "    sentence = tokened_sentence[0:len(tokened_sentence)]\n",
    "    return sentence    \n",
    "\n",
    "def verify(sentence, show_details=False):\n",
    "    bag=[0]*len(all_words)\n",
    "    cleaned_sentence = clean_sentence(sentence)\n",
    "    # This line returns the bag of words as 0 or 1 if words in sentence are found in all_words\n",
    "    x = encode_sentence(all_words,cleaned_sentence,bag)\n",
    "    x = np.array(x)\n",
    "    x = x.reshape(x.shape[0],1)\n",
    "    \n",
    "#    print(\"Shape of X is \", x.shape)\n",
    "    if show_details:\n",
    "        print (\"sentence:\", sentence, \"\\n bow:\", x)\n",
    "    # input layer is our encoded sentence\n",
    "    l0 = x\n",
    "    # matrix multiplication of input and hidden layer\n",
    "    l1 = relu(np.dot(W1,l0)+b1)\n",
    "    # output layer\n",
    "    l2 = softmax(np.dot(W2,l1)+b2)\n",
    "    \n",
    "    return l2\n",
    "\n",
    "def classify(sentence, show_details=False):\n",
    "    results = verify(sentence, show_details)\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD ] \n",
    "    results.sort(key=lambda x: x[1], reverse=True) \n",
    "    return_results =[[classes[r[0]],r[1]] for r in results]\n",
    "    print (\"%s \\n classification: %s \\n\" % (sentence, return_results))\n",
    "    return return_results\n",
    "\n",
    "classify(\"I want to kill everyone @Name1 #why?\")\n",
    "classify(\"I am so happy @Name2 #yayyyy\")\n",
    "classify(\"This depression will kill me someday .. i am dying @Name3 #killme\")\n",
    "classify(\"I am afraid terrorists might attack us @Name4 #isis\")\n",
    "classify(\"What should I do wh\n",
    "en i am happy @Name5 \")\n",
    "classify(\"I want to be happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "input_sentiment = input(\"Hi :) How are you feeling today ? \")\n",
    "#print(input_sentiment)\n",
    "#print(classify(input_sentiment)[0][0])\n",
    "sentiment = classify(input_sentiment)[0][0]\n",
    "print(sentiment)\n",
    "if sentiment == \"anger\" or sentiment == \"sadness\" or sentiment == \"fear\":\n",
    "    answer = input(\"Sorry to hear that .... would you like to hear a joke to lighten your mood ? Press Yes or No \")\n",
    "    if answer == \"N\" or answer == \"No\" or answer == \"no\" or answer == \"n\":\n",
    "        print(\"Have a nice day. Goodbye :) \")\n",
    "    else:\n",
    "        file = open(r'C:\\Users\\HP\\Documents\\s4--prjct-sample\\Depression-Assistant-Chatbot-master/jokes.txt')\n",
    "        while(1):\n",
    "            full_file = file.readline()\n",
    "            split_file = full_file.split('/')\n",
    "        #    print(split_file)\n",
    "            slashes = full_file.count('/')\n",
    "        #    print(slashes)\n",
    "            line_of_joke = []\n",
    "            for i in range(slashes):\n",
    "                k=0\n",
    "            #    print(split_file[i])\n",
    "                commas = split_file[i].count('\"')\n",
    "        #        print(commas)\n",
    "                length = int(commas/2)\n",
    "                if length == 0:\n",
    "                    line_of_joke.append(split_file[i])\n",
    "                else:\n",
    "                    for j in range(length):\n",
    "        #            print(\"Here\")\n",
    "                        line_of_joke.append(split_file[i].split('\"')[k]+split_file[i].split('\"')[k+1])\n",
    "                        if j==length-1:\n",
    "                            line_of_joke.append(split_file[i].split('\"')[k+2])\n",
    "                        k=k+2\n",
    "    #    break\n",
    "            for i in line_of_joke:\n",
    "                print(i)\n",
    "            user_input = input(\"Do you want another joke ? Write Yes or No\\t\")\n",
    "            if user_input == \"Y\":\n",
    "                clear_output()\n",
    "                pass\n",
    "            else:\n",
    "                clear_output()\n",
    "                break\n",
    "        #print(line_of_joke[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
